{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokyo_coffee_wordcloud.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1sflDs4Bg9-C6C17LH8_p6JhpyMt96vyp",
      "authorship_tag": "ABX9TyO0pWQ4UeQ6BRdNzjKhqiqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaka-honda/lesson_ai/blob/master/tokyo_coffee_wordcloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdNpj-6V2Fym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "46e5ccad-8f2e-432f-ab6d-3838567eb532"
      },
      "source": [
        "# CSVファイルの読込\n",
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/My Drive/Coffee/review.csv')\n",
        "\n",
        "# 店ごとのレビューを取得する為、indexを'store_id'に変更\n",
        "data.set_index('store_id', inplace=True)\n",
        "data.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>store_name</th>\n",
              "      <th>score</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>store_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>カフェ・ド・ランブル</td>\n",
              "      <td>3.76</td>\n",
              "      <td>この日は新橋周辺へ。同級生の烏丸大路に片思いするごく普通の女子高校生・塚本天満、その天満に片...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>カフェ・ド・ランブル</td>\n",
              "      <td>3.76</td>\n",
              "      <td>日本の自家焙煎珈琲店のルーツであるコチラ。定温の専用倉庫で、10年以上保管・熟成した生豆を『...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>カフェ・ド・ランブル</td>\n",
              "      <td>3.76</td>\n",
              "      <td>2019/6月下旬。    自分の中の銀座3大カフェの1軒でした。    勝手にみゆき館、壹...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>カフェ・ド・ランブル</td>\n",
              "      <td>3.76</td>\n",
              "      <td>銀ブラの語源は銀座でブラジルコーヒーを飲むことであるという説をある珈琲店が主張しており、しか...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>カフェ・ド・ランブル</td>\n",
              "      <td>3.76</td>\n",
              "      <td>喫煙可で狭くて混んでいて､寛げませんが他ではあまり味わえないコーヒーが頂けます。昔は珈琲通で...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          store_name  score                                             review\n",
              "store_id                                                                      \n",
              "1         カフェ・ド・ランブル   3.76  この日は新橋周辺へ。同級生の烏丸大路に片思いするごく普通の女子高校生・塚本天満、その天満に片...\n",
              "1         カフェ・ド・ランブル   3.76  日本の自家焙煎珈琲店のルーツであるコチラ。定温の専用倉庫で、10年以上保管・熟成した生豆を『...\n",
              "1         カフェ・ド・ランブル   3.76  2019/6月下旬。    自分の中の銀座3大カフェの1軒でした。    勝手にみゆき館、壹...\n",
              "1         カフェ・ド・ランブル   3.76  銀ブラの語源は銀座でブラジルコーヒーを飲むことであるという説をある珈琲店が主張しており、しか...\n",
              "1         カフェ・ド・ランブル   3.76  喫煙可で狭くて混んでいて､寛げませんが他ではあまり味わえないコーヒーが頂けます。昔は珈琲通で..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7OCzd7FMwDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 日本語が使えるように日本語フォントの設定\n",
        "!apt-get -y install fonts-ipafont-gothic\n",
        "!ls /usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\n",
        "\n",
        "# 正規化と形態素解析のライブラリをインポート\n",
        "!pip install janome\n",
        "from janome.charfilter import *\n",
        "from janome.analyzer import Analyzer\n",
        "from janome.tokenizer import Tokenizer\n",
        "from janome.tokenfilter import *\n",
        "import re\n",
        "# WordCloudのライブラリをインポート\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RvkXRG7YO_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#　店舗ごとに口コミを正規化、形態素解析して単語リストを作成、可視化\n",
        "def coffee():\n",
        "  texts=data['review'][i]\n",
        "\n",
        "  # janomeのAnalyzerを使うことで、文の分割と単語の正規化をまとめて行うことができる\n",
        "  # 文に対する処理のまとめ\n",
        "  char_filters = [UnicodeNormalizeCharFilter(),         # UnicodeをNFKC(デフォルト)で正規化\n",
        "                  RegexReplaceCharFilter('\\(', ''),     # (を削除\n",
        "                  RegexReplaceCharFilter('\\)', '')      # )を削除\n",
        "                  ]\n",
        "  # 単語に分割\n",
        "  tokenizer = Tokenizer()\n",
        "\n",
        "  #\n",
        "  #  ひらがな・カタカナ・英数字の一文字しか無い単語は削除\n",
        "  #\n",
        "  class OneCharacterReplaceFilter(TokenFilter):\n",
        "\n",
        "      def apply(self, tokens):\n",
        "          for token in tokens:\n",
        "              # 上記のルールの一文字制限で引っかかった場合、その単語を無視\n",
        "              if re.match('^[あ-んア-ンa-zA-Z0-9ー]$', token.surface):\n",
        "                  continue\n",
        "\n",
        "              yield token\n",
        "\n",
        "\n",
        "  # 単語に対する処理のまとめ\n",
        "  token_filters = [CompoundNounFilter(),                          # 名詞が連続する場合は複合名詞にする\n",
        "                  POSKeepFilter(['名詞', '形容詞', '副詞']),      # 名詞・形容詞・副詞のみを取得する\n",
        "                  LowerCaseFilter(),                              # 英字は小文字にする\n",
        "                  OneCharacterReplaceFilter()                     # 一文字しか無いひらがなとカタカナと英数字は削除\n",
        "                  ]\n",
        "\n",
        "  analyzer = Analyzer(char_filters, tokenizer, token_filters)\n",
        "\n",
        "  tokens_list = []\n",
        "  raw_texts = []\n",
        "  for text in texts:\n",
        "      # 文を分割し、単語をそれぞれ正規化する\n",
        "      text_ = [token.base_form for token in analyzer.analyze(text)]\n",
        "      if len(text_) > 0:\n",
        "          tokens_list.append([token.base_form for token in analyzer.analyze(text)])\n",
        "          raw_texts.append(text)\n",
        "\n",
        "\n",
        "  # 正規化された際に一文字もない文の削除後の元テキストデータ\n",
        "  raw_texts = [text_+'\\n' for text_ in raw_texts]\n",
        "\n",
        "  # 単語リストの作成\n",
        "  words = []\n",
        "  for text in tokens_list:\n",
        "      words.extend([word+'\\n' for word in text if word != ''])\n",
        "\n",
        "  # 単語リストをstring型に変換して繋げる\n",
        "  review_text = ' '.join(words).replace('\\n', '')\n",
        "\n",
        "  \n",
        "  fpath = \"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\"\n",
        "\n",
        "  # ストップワードの設定\n",
        "  # ここで設定した単語はWord Cloudに表示されない\n",
        "  stop_words = [u'コーヒー', u'珈琲', u'カフェ', u'美味しい', u'雰囲気', u'てる', u'いる', u'なる', u'れる', u'する', u'ある', u'こと', u'これ', u'さん', u'して', \\\n",
        "              u'くれる', u'やる', u'くださる', u'そう', u'せる', u'した', u'思う', \\\n",
        "              u'それ', u'ここ', u'ちゃん', u'くん', u'', u'て', u'に', u'を', u'は', u'の', u'が', u'と', u'た', u'し', u'で', \\\n",
        "              u'ない', u'も', u'な', u'い', u'か', u'ので', u'よう', u'', u'もの', u'もつ']\n",
        "\n",
        "  wordcloud = WordCloud(background_color=\"white\",\n",
        "                      font_path=fpath,\n",
        "                      width=900,\n",
        "                      height=500,\n",
        "                      stopwords=set(stop_words)).generate(review_text)\n",
        "\n",
        "  my_path='/content/drive/My Drive/Coffee/wordcloud/'\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(my_path + store[i-1]+'.png')\n",
        "  # plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTWFu8-R0bN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 'store_name'を格納したSeriesを作成\n",
        "store=pd.Series(data['store_name'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26TypmQT59FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,579):\n",
        "  coffee()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}