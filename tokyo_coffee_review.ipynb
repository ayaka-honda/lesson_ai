{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokyo_coffee_review.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnH6UuPRu4yQ/czk5ErQYM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaka-honda/lesson_ai/blob/master/tokyo_coffee_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joh5rD4bLFcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCkCJsYi5qkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tabelog:\n",
        "    \"\"\"\n",
        "    食べログスクレイピングクラス\n",
        "    test_mode=Trueで動作させると、最初のページの３店舗のデータのみを取得できる\n",
        "    \"\"\"\n",
        "    def __init__(self, base_url, test_mode=False, p_ward='東京都内', begin_page=1, end_page=50):\n",
        "\n",
        "        # 変数宣言\n",
        "        self.store_id = ''\n",
        "        self.store_id_num = 0\n",
        "        self.store_name = ''\n",
        "        self.score = 0\n",
        "        self.ward = p_ward\n",
        "        self.review_cnt = 0\n",
        "        self.review = ''\n",
        "        self.columns = ['store_id', 'store_name', 'score', 'ward', 'review_cnt', 'review']\n",
        "        self.df = pd.DataFrame(columns=self.columns)\n",
        "        self.__regexcomp = re.compile(r'\\n|\\s') # \\nは改行、\\sは空白\n",
        "\n",
        "        page_num = begin_page # 店舗一覧ページ番号\n",
        "\n",
        "        if test_mode:\n",
        "            list_url = base_url + str(page_num) +  '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
        "            self.scrape_list(list_url, mode=test_mode)\n",
        "        else:\n",
        "            while True:\n",
        "                list_url = base_url + str(page_num) +  '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
        "                if self.scrape_list(list_url, mode=test_mode) != True:\n",
        "                    break\n",
        "\n",
        "                # INパラメータまでのページ数データを取得する\n",
        "                if page_num >= end_page:\n",
        "                    break\n",
        "                page_num += 1\n",
        "        return\n",
        "\n",
        "    def scrape_list(self, list_url, mode):\n",
        "        \"\"\"\n",
        "        店舗一覧ページのパーシング\n",
        "        \"\"\"\n",
        "        r = requests.get(list_url)\n",
        "        if r.status_code != requests.codes.ok:\n",
        "            return False\n",
        "\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        soup_a_list = soup.find_all('a', class_='list-rst__rst-name-target') # 店名一覧\n",
        "\n",
        "        if len(soup_a_list) == 0:\n",
        "            return False\n",
        "\n",
        "        if mode:\n",
        "            for soup_a in soup_a_list[:2]:\n",
        "                item_url = soup_a.get('href') # 店の個別ページURLを取得\n",
        "                self.store_id_num += 1\n",
        "                self.scrape_item(item_url, mode)\n",
        "        else:\n",
        "            for soup_a in soup_a_list:\n",
        "                item_url = soup_a.get('href') # 店の個別ページURLを取得\n",
        "                self.store_id_num += 1\n",
        "                self.scrape_item(item_url, mode)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def scrape_item(self, item_url, mode):\n",
        "        \"\"\"\n",
        "        個別店舗情報ページのパーシング\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "\n",
        "        r = requests.get(item_url)\n",
        "        if r.status_code != requests.codes.ok:\n",
        "            print(f'error:not found{ item_url }')\n",
        "            return\n",
        "\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "        # 店舗名称取得\n",
        "        # <h2 class=\"display-name\">\n",
        "        #     <span>\n",
        "        #         COFFEE VALLEY\n",
        "        #     </span>\n",
        "        # </h2>\n",
        "        store_name_tag = soup.find('h2', class_='display-name')\n",
        "        store_name = store_name_tag.span.string\n",
        "        print('{}→店名：{}'.format(self.store_id_num, store_name.strip()), end='')\n",
        "        self.store_name = store_name.strip()\n",
        "\n",
        "        # コーヒー専門店以外の店舗は除外\n",
        "        store_head = soup.find('div', class_='rdheader-subinfo') # 店舗情報のヘッダー枠データ取得\n",
        "        store_head_list = store_head.find_all('dl')\n",
        "        store_head_list = store_head_list[1].find_all('span')\n",
        "        #print('ターゲット：', store_head_list[0].text)\n",
        "\n",
        "        if store_head_list[0].text not in {'コーヒー専門店'}:\n",
        "            print('コーヒー専門店ではないので処理対象外')\n",
        "            self.store_id_num -= 1\n",
        "            return\n",
        "\n",
        "        # 評価点数取得\n",
        "        # <b class=\"c-rating__val rdheader-rating__score-val\" rel=\"v:rating\">\n",
        "        #             <span class=\"rdheader-rating__score-val-dtl\">3.26</span>\n",
        "        # </b>\n",
        "        rating_score_tag = soup.find('b', class_='c-rating__val')\n",
        "        rating_score = rating_score_tag.span.string\n",
        "        print('  評価点数：{}点'.format(rating_score), end='')\n",
        "        self.score = rating_score\n",
        "\n",
        "        # 評価点数が存在しない店舗は除外\n",
        "        if rating_score == '-':\n",
        "            print('  評価がないため処理対象外')\n",
        "            self.store_id_num -= 1\n",
        "            return\n",
        "       # 評価が3.0未満店舗は除外\n",
        "        if float(rating_score) < 3.0:\n",
        "            print('  食べログ評価が3.0未満のため処理対象外')\n",
        "            self.store_id_num -= 1\n",
        "            return\n",
        "\n",
        "        # レビュー一覧URL取得\n",
        "        #<a class=\"mainnavi\" href=\"https://tabelog.com/tokyo/A1303/A130301/13169068/dtlrvwlst/\"><span>口コミ</span><span class=\"rstdtl-navi__total-count\"><em>30</em></span></a>\n",
        "        review_tag_id = soup.find('li', id=\"rdnavi-review\")\n",
        "        review_tag = review_tag_id.a.get('href')\n",
        "\n",
        "        # レビュー件数取得\n",
        "        print('  レビュー件数：{}'.format(review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string), end='')\n",
        "        self.review_cnt = review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string\n",
        "\n",
        "        # レビュー一覧ページ番号\n",
        "        #1ページ*20 = 20レビュー 。この数字を変えて取得するレビュー数を調整。\n",
        "        page_num = 1 \n",
        "\n",
        "        # レビュー一覧ページから個別レビューページを読み込み、パーシング\n",
        "        # 店舗の全レビューを取得すると、食べログの評価ごとにデータ件数の濃淡が発生してしまうため、\n",
        "        # 取得するレビュー数は１ページ分としている（件数としては１ページ*20=２0レビュー）\n",
        "        while True:\n",
        "            review_url = review_tag + 'COND-0/smp1/?lc=0&rvw_part=all&PG=' + str(page_num)\n",
        "            #print('\\t口コミ一覧リンク：{}'.format(review_url))\n",
        "            print(' . ' , end='') #LOG\n",
        "            if self.scrape_review(review_url) != True:\n",
        "                break\n",
        "            if page_num >= 1:\n",
        "                break\n",
        "            page_num += 1\n",
        "\n",
        "        process_time = time.time() - start\n",
        "        print('  取得時間：{}'.format(process_time))\n",
        "\n",
        "        return\n",
        "\n",
        "    def scrape_review(self, review_url):\n",
        "        \"\"\"\n",
        "        レビュー一覧ページのパーシング\n",
        "        \"\"\"\n",
        "        r = requests.get(review_url)\n",
        "        if r.status_code != requests.codes.ok:\n",
        "            print(f'error:not found{ review_url }')\n",
        "            return False\n",
        "\n",
        "        # 各個人の口コミページ詳細へのリンクを取得する\n",
        "        #<div class=\"rvw-item js-rvw-item-clickable-area\" data-detail-url=\"/tokyo/A1304/A130401/13141542/dtlrvwlst/B408082636/?use_type=0&amp;smp=1\">\n",
        "        #</div>\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        review_url_list = soup.find_all('div', class_='rvw-item') # 口コミ詳細ページURL一覧\n",
        "\n",
        "        if len(review_url_list) == 0:\n",
        "            return False\n",
        "\n",
        "        for url in review_url_list:\n",
        "            review_detail_url = 'https://tabelog.com' + url.get('data-detail-url')\n",
        "            #print('\\t口コミURL：', review_detail_url)\n",
        "\n",
        "            # 口コミのテキストを取得\n",
        "            self.get_review_text(review_detail_url)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_review_text(self, review_detail_url):\n",
        "        \"\"\"\n",
        "        口コミ詳細ページをパーシング\n",
        "        \"\"\"\n",
        "        r = requests.get(review_detail_url)\n",
        "        if r.status_code != requests.codes.ok:\n",
        "            print(f'error:not found{ review_detail_url }')\n",
        "            return\n",
        "\n",
        "        # ２回以上来訪してコメントしているユーザは最新の1件のみを採用\n",
        "        #<div class=\"rvw-item__rvw-comment\" property=\"v:description\">\n",
        "        #   <p>\n",
        "        #     池袋のカフェ。<br />駅から6,7分のところにあります。<br />屯ちんのすぐ隣です。<br />日曜16:00ごろ行ったら5,6組待ちだったので、<br />18:00ごろリトライし、無事に入れました。<br />3階建なのでそこそ...\n",
        "        #   </p>\n",
        "        #</div>\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        review = soup.find_all('div', class_='rvw-item__rvw-comment')#reviewが含まれているタグの中身をすべて取得\n",
        "        if len(review) == 0:\n",
        "            review = ''\n",
        "        else:\n",
        "            review = review[0].p.text.strip() # strip()は改行コードを除外する関数\n",
        "\n",
        "        #print('\\t\\t口コミテキスト：', review)\n",
        "        self.review = review\n",
        "\n",
        "        # データフレームの生成\n",
        "        self.make_df()\n",
        "        return\n",
        "\n",
        "    def make_df(self):\n",
        "        self.store_id = str(self.store_id_num).zfill(8) #0パディング\n",
        "        se = pd.Series([self.store_id, self.store_name, self.score, self.ward, self.review_cnt, self.review], self.columns) # 行を作成\n",
        "        self.df = self.df.append(se, self.columns) # データフレームに行を追加\n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnVvVG01MhD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1201b45f-1be9-4ee3-e3f4-ac07b2da8b12"
      },
      "source": [
        "tokyo_coffee_review = Tabelog(base_url=\"https://tabelog.com/tokyo/rstLst/CC03/\",test_mode=False, p_ward='東京都内')\n",
        "#CSV保存\n",
        "tokyo_coffee_review.df.to_csv(\"coffee_review.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1→店名：ハリッツ 上原店コーヒー専門店ではないので処理対象外\n",
            "1→店名：カフェ・ド・ランブル  評価点数：3.76点  レビュー件数：285 .   取得時間：43.3211932182312\n",
            "2→店名：Cafe Bach  評価点数：3.75点  レビュー件数：251 .   取得時間：42.4907066822052\n",
            "3→店名：タイズ  評価点数：3.75点  レビュー件数：171 .   取得時間：42.53116464614868\n",
            "4→店名：茶亭 羽當  評価点数：3.74点  レビュー件数：353 .   取得時間：42.33553409576416\n",
            "5→店名：コヒア アラビカ  評価点数：3.73点  レビュー件数：145 . "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}